{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a474c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets\\\\astronaut.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21336/1353783854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"savefig.bbox\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tight'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0morig_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'assets'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'astronaut.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# if you change the seed, make sure that the randomly-applied transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# properly show that the image can be both transformed and *not* transformed!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\1-program\\2-productivity\\6-anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2974\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2975\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets\\\\astronaut.jpg'"
     ]
    }
   ],
   "source": [
    "# sphinx_gallery_thumbnail_path = \"../../gallery/assets/transforms_thumbnail.png\"\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "orig_img = Image.open(Path('assets') / 'astronaut.jpg')\n",
    "# if you change the seed, make sure that the randomly-applied transforms\n",
    "# properly show that the image can be both transformed and *not* transformed!\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [orig_img] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2cef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a83bf33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.1526, 8.6233, 1.7731, 8.2232, 6.6465, 8.0089, 4.2311, 7.1863],\n",
       "         [8.7594, 1.7874, 7.2812, 1.8446, 8.4852, 9.5051, 0.9142, 4.2815],\n",
       "         [4.0475, 5.4800, 4.9905, 3.1808, 5.3387, 6.8324, 7.1274, 8.4269],\n",
       "         [0.8371, 6.0961, 9.2635, 4.7462, 1.1545, 0.4903, 6.0291, 7.5536],\n",
       "         [4.1274, 2.1205, 8.7438, 3.9129, 9.1773, 8.0246, 7.7303, 8.0707],\n",
       "         [5.3804, 2.2330, 2.8615, 8.3441, 9.5709, 2.8890, 1.9017, 6.8373],\n",
       "         [7.3698, 6.3261, 2.4318, 1.8323, 8.6918, 3.0025, 9.2144, 0.9011],\n",
       "         [5.0799, 7.7298, 5.4969, 4.5555, 0.4346, 8.0101, 8.8896, 9.9460]],\n",
       "\n",
       "        [[6.7365, 8.8141, 5.1034, 1.5474, 7.5257, 6.9479, 4.7057, 5.1526],\n",
       "         [5.4651, 8.8796, 1.9432, 8.8625, 3.8524, 0.2306, 9.2425, 1.9612],\n",
       "         [9.6590, 8.6857, 9.2598, 9.1804, 4.0736, 1.0818, 2.9418, 5.8012],\n",
       "         [2.7771, 7.5689, 2.9397, 2.4829, 0.8491, 9.4553, 7.6330, 9.0288],\n",
       "         [2.5065, 7.4430, 9.4390, 2.0838, 3.5115, 0.6197, 9.0202, 4.4726],\n",
       "         [2.4121, 2.0853, 9.8786, 4.2774, 7.4532, 4.2167, 4.3241, 5.2959],\n",
       "         [6.8669, 9.3532, 5.5597, 5.7063, 8.4104, 8.3741, 0.1762, 2.1160],\n",
       "         [2.3970, 7.0931, 0.1057, 5.1666, 5.2120, 2.2234, 1.5830, 4.9305]],\n",
       "\n",
       "        [[7.6634, 3.6583, 4.5485, 3.7121, 5.1312, 5.9516, 4.2836, 9.8364],\n",
       "         [8.1811, 0.2837, 0.9544, 4.4995, 9.1798, 9.4690, 5.7777, 3.4603],\n",
       "         [1.8510, 4.9619, 8.4242, 0.0957, 7.5321, 6.9963, 6.3554, 0.5358],\n",
       "         [5.5872, 6.2178, 1.2701, 0.8434, 9.9707, 9.0787, 2.5148, 5.7403],\n",
       "         [5.9824, 0.1542, 5.0524, 4.7695, 4.4281, 7.8549, 6.7780, 9.8188],\n",
       "         [3.0835, 8.7934, 6.5186, 8.1474, 2.5626, 9.9598, 8.3010, 4.5462],\n",
       "         [7.0068, 2.1964, 0.3272, 9.2785, 9.6011, 1.5002, 8.1829, 0.8385],\n",
       "         [7.9172, 1.9507, 0.2437, 3.5552, 1.8882, 9.6093, 0.8493, 4.2698]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(3,8,8)\n",
    "a*=10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf8ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FigureDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.Data=torch.tensor([[1,1],[2,2],[3,3],[4,4]])\n",
    "        self.Label=torch.tensor([9,8,7,6])\n",
    "    def __getitem__(self, index):\n",
    "        image=self.Data[index]\n",
    "        label=self.Label[index]\n",
    "        return image, label\n",
    "    def __len__(self):\n",
    "        return len(self.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "918a7176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [2., 4., 6.]],\n",
       "\n",
       "        [[2., 3., 4.],\n",
       "         [4., 5., 6.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=torch.ones(2,2,3)\n",
    "aa[:,:,1:2]+=1\n",
    "aa[:,:,2:3]+=2\n",
    "aa[1,0,:]+=1\n",
    "aa[0,1,:]*=2\n",
    "aa[1,1,:]+=3\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e1d6730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0028664",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1=torch.nn.Sequential(\n",
    "    transforms.Normalize((0, 0.5), (1, 0.5))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b48e0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "     [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0bf9911e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.],\n",
       "         [ 2.,  4.,  6.]],\n",
       "\n",
       "        [[ 3.,  5.,  7.],\n",
       "         [ 7.,  9., 11.]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=transform1(aa)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c21be345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3196)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b113d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6393)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ae195c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.3053, 16.2465,  2.5461, 15.4464, 12.2930, 15.0177,  7.4622,\n",
       "          13.3726],\n",
       "         [16.5187,  2.5749, 13.5625,  2.6892, 15.9705, 18.0101,  0.8283,\n",
       "           7.5629],\n",
       "         [ 7.0949,  9.9601,  8.9809,  5.3616,  9.6775, 12.6648, 13.2549,\n",
       "          15.8539],\n",
       "         [ 0.6742, 11.1923, 17.5270,  8.4924,  1.3089, -0.0193, 11.0583,\n",
       "          14.1071],\n",
       "         [ 7.2548,  3.2410, 16.4877,  6.8258, 17.3546, 15.0493, 14.4607,\n",
       "          15.1415],\n",
       "         [ 9.7607,  3.4661,  4.7230, 15.6882, 18.1418,  4.7781,  2.8034,\n",
       "          12.6746],\n",
       "         [13.7396, 11.6522,  3.8637,  2.6647, 16.3836,  5.0051, 17.4288,\n",
       "           0.8022],\n",
       "         [ 9.1597, 14.4596,  9.9938,  8.1110, -0.1309, 15.0202, 16.7792,\n",
       "          18.8921]],\n",
       "\n",
       "        [[12.4730, 16.6282,  9.2067,  2.0949, 14.0513, 12.8958,  8.4114,\n",
       "           9.3052],\n",
       "         [ 9.9301, 16.7591,  2.8864, 16.7251,  6.7048, -0.5388, 17.4850,\n",
       "           2.9225],\n",
       "         [18.3180, 16.3713, 17.5195, 17.3607,  7.1472,  1.1636,  4.8837,\n",
       "          10.6024],\n",
       "         [ 4.5542, 14.1378,  4.8793,  3.9658,  0.6981, 17.9107, 14.2661,\n",
       "          17.0576],\n",
       "         [ 4.0131, 13.8859, 17.8780,  3.1676,  6.0229,  0.2395, 17.0404,\n",
       "           7.9452],\n",
       "         [ 3.8243,  3.1707, 18.7572,  7.5548, 13.9063,  7.4334,  7.6483,\n",
       "           9.5917],\n",
       "         [12.7338, 17.7064, 10.1194, 10.4125, 15.8208, 15.7482, -0.6476,\n",
       "           3.2320],\n",
       "         [ 3.7939, 13.1861, -0.7885,  9.3332,  9.4239,  3.4467,  2.1659,\n",
       "           8.8610]],\n",
       "\n",
       "        [[14.3269,  6.3166,  8.0970,  6.4242,  9.2625, 10.9033,  7.5672,\n",
       "          18.6728],\n",
       "         [15.3622, -0.4326,  0.9088,  7.9989, 17.3597, 17.9380, 10.5555,\n",
       "           5.9205],\n",
       "         [ 2.7020,  8.9237, 15.8484, -0.8085, 14.0642, 12.9926, 11.7108,\n",
       "           0.0717],\n",
       "         [10.1744, 11.4355,  1.5402,  0.6869, 18.9415, 17.1575,  4.0296,\n",
       "          10.4806],\n",
       "         [10.9649, -0.6916,  9.1048,  8.5391,  7.8563, 14.7097, 12.5560,\n",
       "          18.6377],\n",
       "         [ 5.1669, 16.5869, 12.0372, 15.2948,  4.1252, 18.9196, 15.6019,\n",
       "           8.0924],\n",
       "         [13.0136,  3.3928, -0.3456, 17.5570, 18.2022,  2.0003, 15.3658,\n",
       "           0.6770],\n",
       "         [14.8345,  2.9013, -0.5125,  6.1104,  2.7764, 18.2185,  0.6987,\n",
       "           7.5396]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe07332f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21336/4004044404.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "torch.nn.transforms.ToTensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba8e81f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21336/3952999657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFDataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFigureDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "FDataset=FigureDataset()\n",
    "loader=DataLoader(FDataset,batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
